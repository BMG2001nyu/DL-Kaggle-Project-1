{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b8528d-687d-4b96-b391-abffb4f82343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b4a2c2-f3b4-420e-80c1-2975b34260ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b563084-86dc-4cfa-8ba5-ef35fc1c2dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d5808",
   "metadata": {},
   "source": [
    "load_pickle() => is used to read the CIFAR 10 dataset and stores in binary format \n",
    "\n",
    "Data files are loaded in 5 batches, then the images and label's were extracted and were transformed from (C,H,W) to (H,W,C) using transpose(1,2,0)\n",
    "\n",
    "Data augmentation and normalization was done using tensor, mean and standard deviation to stablize and speedup training. \n",
    "\n",
    "Images were randomly cropeed to 32x32 pixels and padding of 4 pixels was added to help in training so that images with slight shifts were also classified correctly \n",
    "\n",
    "DataLoader is used to help the model in learning data in random instead of leaning in a particular order every epoch, 4 worker threads were used to speedup the training process by parallelizing it. \n",
    "\n",
    "Batch size of 256 is used to process many samples of data simultaneously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa722fd1-d642-4055-a5b9-52f1872fa9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "batch_files = [\n",
    "    \"./data_batch_1\",\n",
    "    \"./data_batch_2\",\n",
    "    \"./data_batch_3\",\n",
    "    \"./data_batch_4\",\n",
    "    \"./data_batch_5\"\n",
    "]\n",
    "\n",
    "train_data, train_labels = [], []\n",
    "for file in batch_files:\n",
    "    batch = load_pickle(file)\n",
    "    train_data.append(batch[b'data'])       \n",
    "    train_labels.extend(batch[b'labels'])\n",
    "\n",
    "train_data = np.vstack(train_data).reshape(-1, 3, 32, 32)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].transpose(1, 2, 0)  \n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "# Use your existing transform (update if needed)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# Initialize dataset and loader\n",
    "trainset = CIFAR10Dataset(train_data, train_labels, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14f50f",
   "metadata": {},
   "source": [
    "mish activation function is used to provide smooth gradients and improving the convergence of the network\n",
    "Mish(x) = x . tanh(ln(1+e^x))\n",
    "\n",
    "SE block generates attention weights which scales the output of the convolutional layers\n",
    "\n",
    "Basic block has 2 convolutional layers with batch normalization and mish activation, skip connection is added to allow the flow of gradients solving the vanishing gradient problem\n",
    "\n",
    "\n",
    "Modified ResNet18 - there are 4 major layers to capture different levels of abstraction that is built using stacking the basic blocks\n",
    "\n",
    "Adaptive Average Pooling layers compresses the spatial dimensions into 1x1 per channel - used to summerize each feature map into a single value per channel\n",
    "\n",
    "Normalization is added here as well for stability and speeding up the training \n",
    "\n",
    "\n",
    "While training the model all the transformations and conversions that were used while reading and procressing was added here.\n",
    "\n",
    "Using cross entropy loss to evaluate how well it will align with the prediction \n",
    "\n",
    "SGD is used for stable learning over time \n",
    "\n",
    "Weight decay is used to reguarlize the model, tried with 5e-3, 1e-3\n",
    "\n",
    "CosineAnnealingWarmRestarts is used to dynamically change learning rate based on the loss and number of epochs \n",
    "\n",
    "Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd11f9c-1891-4bd6-871c-00dbecb15cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            Mish(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SEBlock(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Mish()(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        out += self.shortcut(x)\n",
    "        return Mish()(out)\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, dropout_prob=0.5):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_planes = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Mish()(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = F.dropout(out, p =0.5, training=self.training)\n",
    "        out = torch.flatten(out, 1)\n",
    "        return self.fc(out)\n",
    "\n",
    "def train():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.AutoAugment(),\n",
    "        transforms.ToTensor(),          \n",
    "        transforms.RandomErasing(),     \n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "    trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "    model = ResNet18(BasicBlock, [2, 2, 2, 2]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=1e-3)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        for inputs, labels in tqdm(trainloader, desc=f\"Epoch {epoch+1}/200\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += outputs.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}: Loss {total_loss/len(trainloader):.4f} | Acc: {100.*correct/len(trainset):.2f}%')\n",
    "        torch.save(model.state_dict(), \"1optimized_resnet18.pth\")\n",
    "        \n",
    "    print(\"\\n🔍 Model Summary After 100 Epochs:\")\n",
    "    summary(model, (3, 32, 32))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a75f3f-0c2b-43aa-b6f8-3549906f4a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "✅ Test set loaded: (10000, 32, 32, 3)\n",
      "✅ Inference completed!\n",
      "submission.csv generated! You can submit this file to Kaggle.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#from model import ResNet18CIFAR  # Ensure that ResNet18CIFAR is defined in model.py\n",
    "\n",
    "# ✅ Step 1: Load the trained model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Instantiate the model architecture\n",
    "model = ResNet18(BasicBlock, [2, 2, 2, 2]).to(device)\n",
    "# Load the state dictionary and update the model\n",
    "state_dict = torch.load(\"optimized_resnet18.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "# ✅ Step 2: Load CIFAR test data (without labels)\n",
    "def load_cifar_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        batch = pickle.load(fo, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "file_path = \"./cifar_test_nolabel.pkl\"\n",
    "cifar10_batch = load_cifar_batch(file_path)\n",
    "\n",
    "test_images = cifar10_batch[b'data']  # Already in (N, 32, 32, 3) format\n",
    "image_ids = cifar10_batch[b'ids']  # Extract image IDs\n",
    "\n",
    "print(f\"✅ Test set loaded: {test_images.shape}\")\n",
    "\n",
    "# ✅ Step 3: Convert test images to PyTorch tensors\n",
    "# Convert to tensor and rearrange dimensions from (N, 32, 32, 3) to (N, C, H, W)\n",
    "test_images = torch.tensor(test_images, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "\n",
    "# Define normalization (same as training)\n",
    "transform = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
    "\n",
    "# Apply normalization to each image\n",
    "test_images = torch.stack([transform(img) for img in test_images])\n",
    "\n",
    "# ✅ Step 4: Run inference\n",
    "test_images = test_images.to(device)  # Move to GPU if available\n",
    "batch_size = 64  # Adjust batch size if needed\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_images), batch_size):\n",
    "        batch = test_images[i:i + batch_size]  # Get batch\n",
    "        outputs = model(batch)  # Forward pass\n",
    "        _, predicted_labels = torch.max(outputs, 1)  # Get predicted labels\n",
    "        predictions.extend(predicted_labels.cpu().numpy())  # Convert to list\n",
    "\n",
    "print(\"✅ Inference completed!\")\n",
    "\n",
    "# ✅ Step 5: Save predictions to CSV\n",
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": image_ids,\n",
    "    \"Labels\": predictions\n",
    "})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv generated! You can submit this file to Kaggle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b299f09-6e9b-4dce-af53-15bfc2088401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  Labels\n",
      "0   0       6\n",
      "1   1       1\n",
      "2   2       8\n",
      "3   3       6\n",
      "4   4       9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"./submission.csv\")\n",
    "\n",
    "# Display first few rows to understand structure\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d456a19c-c5db-4371-86e8-5c0b60624f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique labels: 10\n",
      "Labels\n",
      "8    1200\n",
      "9    1151\n",
      "3    1111\n",
      "7    1050\n",
      "5    1045\n",
      "4     958\n",
      "2     954\n",
      "6     937\n",
      "1     840\n",
      "0     754\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count unique labels\n",
    "unique_labels = df['Labels'].nunique()\n",
    "print(f\"Total unique labels: {unique_labels}\")\n",
    "\n",
    "# Show each unique label with their counts\n",
    "label_counts = df['Labels'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "218c1f64-0b1d-47c9-a89d-bc626ddaedd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "✅ Test set loaded: (10000, 3072)\n",
      "Accuracy on test set: 93.76%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "\n",
    "def load_cifar_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        batch = pickle.load(fo, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "# ✅ Load the trained model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = ResNet18(BasicBlock, [2, 2, 2, 2]).to(device)\n",
    "state_dict = torch.load(\"optimized_resnet18.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "# ✅ Load the labeled CIFAR test dataset\n",
    "file_path = \"test_batch\"  # Adjust path as needed\n",
    "cifar_batch = load_cifar_batch(file_path)\n",
    "\n",
    "# Assuming the file contains keys: b'data' and b'labels'\n",
    "test_images = cifar_batch[b'data']    # Expected shape: (N, 32, 32, 3) or (N, 3072)\n",
    "true_labels = cifar_batch[b'labels']   # Ground truth labels\n",
    "\n",
    "print(f\"✅ Test set loaded: {test_images.shape}\")\n",
    "\n",
    "# ✅ Convert test images to PyTorch tensors\n",
    "# Check if images are flattened (N, 3072) and reshape if needed\n",
    "if test_images.shape[1] == 3072:\n",
    "    # Reshape to (N, 3, 32, 32)\n",
    "    test_images = test_images.reshape(-1, 3, 32, 32)\n",
    "elif test_images.shape[1] == 32:\n",
    "    # If the images are in (N, 32, 32, 3) format, permute dimensions\n",
    "    test_images = torch.tensor(test_images, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "else:\n",
    "    # Otherwise, assume it's already in a suitable tensor format\n",
    "    test_images = torch.tensor(test_images, dtype=torch.float32) / 255.0\n",
    "\n",
    "# If not already a tensor, convert it\n",
    "if not isinstance(test_images, torch.Tensor):\n",
    "    test_images = torch.tensor(test_images, dtype=torch.float32) / 255.0\n",
    "\n",
    "# If the images are not in (N, C, H, W), permute them accordingly\n",
    "if test_images.shape[1] != 3:\n",
    "    test_images = test_images.permute(0, 3, 1, 2)\n",
    "\n",
    "# Normalize images (using the same values as during training)\n",
    "normalize_transform = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                                             std=[0.2470, 0.2435, 0.2616])\n",
    "\n",
    "test_images = torch.stack([normalize_transform(img) for img in test_images])\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "# Convert true_labels to a tensor and move to the same device\n",
    "true_labels = torch.tensor(true_labels, dtype=torch.long).to(device)\n",
    "\n",
    "# ✅ Run inference and compute accuracy\n",
    "batch_size = 64\n",
    "total = test_images.size(0)\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = test_images[i:i+batch_size]\n",
    "        outputs = model(batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == true_labels[i:i+batch_size]).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ee922-b92b-4037-b171-7d777b2eec39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/200: 100%|██████████| 196/196 [00:15<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101: Loss 0.7867 | Acc: 87.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/200: 100%|██████████| 196/196 [00:12<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102: Loss 0.7901 | Acc: 87.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/200: 100%|██████████| 196/196 [00:12<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103: Loss 0.7848 | Acc: 88.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/200: 100%|██████████| 196/196 [00:12<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104: Loss 0.7724 | Acc: 88.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/200: 100%|██████████| 196/196 [00:12<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105: Loss 0.7611 | Acc: 88.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/200: 100%|██████████| 196/196 [00:12<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106: Loss 0.7436 | Acc: 89.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/200: 100%|██████████| 196/196 [00:12<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107: Loss 0.7275 | Acc: 90.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/200: 100%|██████████| 196/196 [00:12<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108: Loss 0.7149 | Acc: 91.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/200: 100%|██████████| 196/196 [00:12<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109: Loss 0.7129 | Acc: 91.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/200: 100%|██████████| 196/196 [00:13<00:00, 14.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110: Loss 0.7021 | Acc: 91.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/200: 100%|██████████| 196/196 [00:13<00:00, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111: Loss 0.7760 | Acc: 88.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/200: 100%|██████████| 196/196 [00:13<00:00, 14.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112: Loss 0.7842 | Acc: 88.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/200: 100%|██████████| 196/196 [00:13<00:00, 14.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113: Loss 0.7807 | Acc: 88.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/200: 100%|██████████| 196/196 [00:12<00:00, 15.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114: Loss 0.7804 | Acc: 88.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/200: 100%|██████████| 196/196 [00:12<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115: Loss 0.7741 | Acc: 88.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/200: 100%|██████████| 196/196 [00:13<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116: Loss 0.7671 | Acc: 88.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/200: 100%|██████████| 196/196 [00:12<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117: Loss 0.7607 | Acc: 89.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/200: 100%|██████████| 196/196 [00:13<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118: Loss 0.7560 | Acc: 89.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/200: 100%|██████████| 196/196 [00:13<00:00, 14.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119: Loss 0.7478 | Acc: 89.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/200: 100%|██████████| 196/196 [00:12<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120: Loss 0.7380 | Acc: 90.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/200: 100%|██████████| 196/196 [00:12<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121: Loss 0.7264 | Acc: 90.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/200: 100%|██████████| 196/196 [00:12<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122: Loss 0.7247 | Acc: 90.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123/200: 100%|██████████| 196/196 [00:12<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123: Loss 0.7118 | Acc: 91.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124/200: 100%|██████████| 196/196 [00:13<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124: Loss 0.7073 | Acc: 91.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125/200: 100%|██████████| 196/196 [00:12<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125: Loss 0.6969 | Acc: 91.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126/200: 100%|██████████| 196/196 [00:12<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126: Loss 0.6915 | Acc: 92.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127/200: 100%|██████████| 196/196 [00:13<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127: Loss 0.6877 | Acc: 92.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128/200: 100%|██████████| 196/196 [00:13<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128: Loss 0.6845 | Acc: 92.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129/200: 100%|██████████| 196/196 [00:12<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129: Loss 0.6811 | Acc: 92.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130/200: 100%|██████████| 196/196 [00:12<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130: Loss 0.6819 | Acc: 92.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/200: 100%|██████████| 196/196 [00:12<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131: Loss 0.7619 | Acc: 88.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132/200: 100%|██████████| 196/196 [00:12<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132: Loss 0.7717 | Acc: 88.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133/200: 100%|██████████| 196/196 [00:12<00:00, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133: Loss 0.7734 | Acc: 88.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134/200: 100%|██████████| 196/196 [00:12<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134: Loss 0.7745 | Acc: 88.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135/200: 100%|██████████| 196/196 [00:12<00:00, 15.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135: Loss 0.7725 | Acc: 88.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136/200: 100%|██████████| 196/196 [00:12<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136: Loss 0.7694 | Acc: 88.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/200: 100%|██████████| 196/196 [00:12<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137: Loss 0.7694 | Acc: 88.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138/200: 100%|██████████| 196/196 [00:12<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138: Loss 0.7611 | Acc: 89.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139/200: 100%|██████████| 196/196 [00:13<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139: Loss 0.7660 | Acc: 88.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140/200: 100%|██████████| 196/196 [00:12<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140: Loss 0.7564 | Acc: 89.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/200: 100%|██████████| 196/196 [00:12<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141: Loss 0.7537 | Acc: 89.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/200: 100%|██████████| 196/196 [00:12<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142: Loss 0.7496 | Acc: 89.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143/200: 100%|██████████| 196/196 [00:13<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143: Loss 0.7443 | Acc: 89.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144/200: 100%|██████████| 196/196 [00:12<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144: Loss 0.7449 | Acc: 89.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145/200: 100%|██████████| 196/196 [00:12<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145: Loss 0.7403 | Acc: 89.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146/200: 100%|██████████| 196/196 [00:12<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146: Loss 0.7316 | Acc: 90.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147/200: 100%|██████████| 196/196 [00:13<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147: Loss 0.7280 | Acc: 90.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148/200: 100%|██████████| 196/196 [00:12<00:00, 15.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148: Loss 0.7258 | Acc: 90.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149/200: 100%|██████████| 196/196 [00:12<00:00, 15.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: Loss 0.7207 | Acc: 90.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150/200: 100%|██████████| 196/196 [00:12<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150: Loss 0.7137 | Acc: 91.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/200: 100%|██████████| 196/196 [00:12<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151: Loss 0.7144 | Acc: 91.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/200: 100%|██████████| 196/196 [00:12<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152: Loss 0.7097 | Acc: 91.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153/200: 100%|██████████| 196/196 [00:12<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153: Loss 0.7001 | Acc: 91.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154/200: 100%|██████████| 196/196 [00:12<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154: Loss 0.7027 | Acc: 91.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155/200: 100%|██████████| 196/196 [00:12<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155: Loss 0.6920 | Acc: 92.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156/200: 100%|██████████| 196/196 [00:12<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156: Loss 0.6878 | Acc: 92.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157/200: 100%|██████████| 196/196 [00:12<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157: Loss 0.6847 | Acc: 92.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158/200: 100%|██████████| 196/196 [00:12<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158: Loss 0.6796 | Acc: 92.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159/200: 100%|██████████| 196/196 [00:13<00:00, 14.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159: Loss 0.6781 | Acc: 92.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160/200: 100%|██████████| 196/196 [00:13<00:00, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160: Loss 0.6750 | Acc: 92.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/200: 100%|██████████| 196/196 [00:13<00:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161: Loss 0.6703 | Acc: 93.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/200: 100%|██████████| 196/196 [00:13<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162: Loss 0.6654 | Acc: 93.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163/200: 100%|██████████| 196/196 [00:12<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163: Loss 0.6624 | Acc: 93.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164/200: 100%|██████████| 196/196 [00:13<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164: Loss 0.6639 | Acc: 93.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165/200: 100%|██████████| 196/196 [00:12<00:00, 15.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165: Loss 0.6599 | Acc: 93.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166/200: 100%|██████████| 196/196 [00:13<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166: Loss 0.6609 | Acc: 93.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167/200: 100%|██████████| 196/196 [00:13<00:00, 14.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167: Loss 0.6583 | Acc: 93.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168/200: 100%|██████████| 196/196 [00:13<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168: Loss 0.6590 | Acc: 93.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169/200: 100%|██████████| 196/196 [00:13<00:00, 14.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169: Loss 0.6569 | Acc: 93.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170/200: 100%|██████████| 196/196 [00:14<00:00, 13.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170: Loss 0.6606 | Acc: 93.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/200: 100%|██████████| 196/196 [00:13<00:00, 14.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171: Loss 0.7514 | Acc: 89.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/200: 100%|██████████| 196/196 [00:12<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172: Loss 0.7576 | Acc: 89.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173/200: 100%|██████████| 196/196 [00:12<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173: Loss 0.7611 | Acc: 88.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174/200: 100%|██████████| 196/196 [00:12<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174: Loss 0.7583 | Acc: 89.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175/200: 100%|██████████| 196/196 [00:12<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175: Loss 0.7558 | Acc: 89.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176/200: 100%|██████████| 196/196 [00:12<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176: Loss 0.7593 | Acc: 89.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177/200: 100%|██████████| 196/196 [00:12<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177: Loss 0.7556 | Acc: 89.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178/200: 100%|██████████| 196/196 [00:12<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178: Loss 0.7560 | Acc: 89.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179/200: 100%|██████████| 196/196 [00:13<00:00, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179: Loss 0.7546 | Acc: 89.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180/200: 100%|██████████| 196/196 [00:13<00:00, 14.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180: Loss 0.7547 | Acc: 89.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/200: 100%|██████████| 196/196 [00:13<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181: Loss 0.7523 | Acc: 89.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/200: 100%|██████████| 196/196 [00:13<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182: Loss 0.7487 | Acc: 89.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183/200: 100%|██████████| 196/196 [00:13<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183: Loss 0.7446 | Acc: 89.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184/200: 100%|██████████| 196/196 [00:13<00:00, 14.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184: Loss 0.7500 | Acc: 89.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185/200: 100%|██████████| 196/196 [00:13<00:00, 14.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185: Loss 0.7453 | Acc: 89.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186/200: 100%|██████████| 196/196 [00:14<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186: Loss 0.7391 | Acc: 89.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187/200: 100%|██████████| 196/196 [00:13<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187: Loss 0.7435 | Acc: 89.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188/200: 100%|██████████| 196/196 [00:12<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188: Loss 0.7391 | Acc: 89.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189/200: 100%|██████████| 196/196 [00:13<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189: Loss 0.7385 | Acc: 89.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190/200: 100%|██████████| 196/196 [00:13<00:00, 14.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190: Loss 0.7341 | Acc: 90.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191/200: 100%|██████████| 196/196 [00:13<00:00, 14.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191: Loss 0.7288 | Acc: 90.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192/200: 100%|██████████| 196/196 [00:13<00:00, 14.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192: Loss 0.7292 | Acc: 90.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193/200: 100%|██████████| 196/196 [00:12<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193: Loss 0.7305 | Acc: 90.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 194/200: 100%|██████████| 196/196 [00:13<00:00, 14.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194: Loss 0.7321 | Acc: 90.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 195/200: 100%|██████████| 196/196 [00:13<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195: Loss 0.7239 | Acc: 90.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196/200: 100%|██████████| 196/196 [00:13<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196: Loss 0.7227 | Acc: 90.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197/200:  89%|████████▉ | 174/196 [00:11<00:01, 15.39it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "# Mish Activation\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "# SE Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            Mish(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "# Optimized Basic Block\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SEBlock(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Mish()(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)\n",
    "        out += self.shortcut(x)\n",
    "        return Mish()(out)\n",
    "\n",
    "# ResNet18 Modified\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_planes = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Mish()(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = F.dropout(out, p =0.5, training=self.training)\n",
    "        return self.fc(out)\n",
    "\n",
    "def continue_training():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Data augmentation and normalization for training\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.AutoAugment(),\n",
    "        transforms.ToTensor(),          # convert PIL to Tensor first\n",
    "        transforms.RandomErasing(),     # then apply RandomErasing\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "    \n",
    "    # Load CIFAR10 dataset\n",
    "    trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Instantiate the model and load the state from the previous 100 epochs\n",
    "    model = ResNet18(BasicBlock, [2, 2, 2, 2]).to(device)\n",
    "    model.load_state_dict(torch.load(\"optimized_resnet18.pth\"))\n",
    "    \n",
    "    # Define loss function, optimizer, and scheduler\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "    \n",
    "    # Continue training for another 100 epochs (from epoch 100 to 199)\n",
    "    for epoch in range(100, 200):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        for inputs, labels in tqdm(trainloader, desc=f\"Epoch {epoch+1}/200\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += outputs.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}: Loss {total_loss/len(trainloader):.4f} | Acc: {100.*correct/len(trainset):.2f}%')\n",
    "        torch.save(model.state_dict(), \"optimized_resnet18.pth\")\n",
    "        \n",
    "    print(\"\\n🔍 Model Summary After 200 Epochs:\")\n",
    "    summary(model, (3, 32, 32))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    continue_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412c828-2791-4e3a-86a4-5a9ba52abcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
